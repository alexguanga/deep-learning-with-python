{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: What Is Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Artificial intelligence, machine learning, and deep learning\n",
    "- <img src=\"./images/deeplearning_1.png\" width=250px>\n",
    "- Artificial intelligence is the attempt for a computer to automate mundane tasks\n",
    "- Moreover, these task do not have to be \"learning\" but could be set by the programmer with a set of rules\n",
    "- Machine learning takes a unique spin on it. Rather than using certain rules, the rules are created by the computer\n",
    "- You give the model certain data and answers, and machine learning tried to suprise you with the rules that it learned\n",
    "- *I just stated that machine learning discovers rules to execute a data-processing task, given examples of what’s expected.* - pg 6\n",
    "    - *Input data points—For instance, if the task is speech recognition, these data points could be sound files of people speaking. If the task is image tagging, they could be pictures.*\n",
    "    - *Examples of the expected output—In a speech-recognition task, these could be human-generated transcripts of sound files. In an image task, expected outputs could be tags such as “dog,” “cat,” and so on.*\n",
    "    - *A way to measure whether the algorithm is doing a good job—This is necessary in order to determine the distance between the algorithm’s current output and its expected output. The measurement is used as a feedback signal to adjust the way the algorithm works. This adjustment step is what we call learning.*\n",
    "- In dee leanring, the depth refers to the number of layers that the model required to find the answers\n",
    "- Remember, that the weight the deep learning model learns are numbers\n",
    "- The transformation implemented by a layer is parameterized by its weights\n",
    "- Leanring means finding a set of values for the weights of all layers in a network\n",
    "- However, there can be a ot of parameters that the moel will need to figure out\n",
    "- Also, the job of the loss function of the network, also called the objective function is to find how accurate is the parameters that are being used as weights\n",
    "- <img src=\"./images/deeplearning_2.png\" width=250px>\n",
    "- *This adjustment is the job of the optimizer, which implements what’s called the Backpropagation algorithm: the central algorithm in deep learning. The next chapter explains in more detail how backpropagation works.*\n",
    "- Intersting feats accompplished by deep learning:\n",
    "    - Improved text-to-speech conversion\n",
    "    - Digital assistants such as Google Now and Amazon Alexa\n",
    "    - Near-human-level autonomous driving\n",
    "    - Improved ad targeting, as used by Google, Baidu, and Bing\n",
    "    - Improved search results on the web\n",
    "    - Ability to answer natural-language questions\n",
    "    - Superhuman Go playing\n",
    "    - Near-human-level image classification\n",
    "    - Near-human-level speech recognition\n",
    "    - Near-human-level handwriting transcription\n",
    "    - Improved machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Before deep learning: a brief history of machine learning\n",
    "- Probabilistic modeling is the application of the principles of statistics to data analysis. One of the best-known algorithms in this category is the Naive Bayes algorithm.\n",
    "- A closely related model is the logistic regression (logreg for short), which is sometimes considered to be the “hello world” of modern machine learning.\n",
    "- The first successful practical application of neural nets came in 1989 from Bell Labs, when Yann LeCun combined the earlier ideas of convolutional neural networks and backpropagation, and applied them to the problem of classifying handwritten digits.\n",
    "- Kernel methods are a group of classification algorithms, the best known of which is the support vector machine (SVM).\n",
    "    - The modern formulation of an SVM was developed by Vladimir Vapnik and Corinna Cortes in the early 1990s at Bell Labs\n",
    "    - *A kernel function is a computationally tractable operation that maps any two points in your initial space to the distance between these points in your target representation space, completely bypassing the explicit computation of the new representation.*\n",
    "    - But SVMs proved hard to scale to large datasets and didn’t provide good results for perceptual problems such as image classification.\n",
    "- Decision trees are flowchart-like structures that let you classify input data points or predict output values given inputs\n",
    "- Random Forest was one of the most popular models where it used a lot of decision trees\n",
    "- A gradient boosting machine, much like a random forest, is a machine-learning technique based on ensembling weak prediction models, generally decision trees.\n",
    "    - A gradient boosting models works by addresing the weak points of the previous models\n",
    "- but with the rise of deep learning, we learned that deep learning could automate a lot of the feature engineering. Thus, computing the enginnering was automated by deep learning\n",
    "- Things to note is that the XGboost and deep learning are the most important model to be using... they offer the best result. XGBoost is the best for struture data while deep learning offers the best rsults for unstructure data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Why deep learning? Why now?\n",
    "- There are a lot of things that improved: data, hardware, and algos. Algos bagen to improve because they could be developed withh a lot of layers\n",
    "- Simplicity—Deep learning removes the need for feature engineering, replacing complex, brittle, engineering-heavy pipelines with simple, end-to-end trainable models that are typically built using only five or six different tensor operations.\n",
    "- Scalability—Deep learning is highly amenable to parallelization on GPUs or TPUs, so it can take full advantage of Moore’s law. In addition, deep-learning models are trained by iterating over small batches of data, allowing them to be trained on datasets of arbitrary size. (The only bottleneck is the amount of parallel computational power available, which, thanks to Moore’s law, is a fastmoving barrier.)\n",
    "- Versatility and reusability—Unlike many prior machine-learning approaches, deep-learning models can be trained on additional data without restarting from scratch, making them viable for continuous online learning—an important property for very large production models. Furthermore, trained deep-learning models are repurposable and thus reusable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to strucutre the labeling \n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
