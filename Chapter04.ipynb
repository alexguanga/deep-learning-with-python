{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Fundamentals of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Four Branches of machine learning\n",
    "\n",
    "Three types of machine-learning (supervised)\n",
    "1. Binary classification. Here, we only look at the values of yes and no. This coul d be if we have a iimage and are tyring to indentify itf the image is a cat... These typoes of models are not well regarded since it is very limited!\n",
    "2. Multi-class classification. Here, we are constraint by a limited number of outcomes. For ex, we could have the train that are most likely to come. Hence, are possible outcomes cannot be any number. \n",
    "3. Scalar Regression. Here, we are looking at a regular regression problem. This could be houee price or anything that could have a discrete possible outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Supervised learning\n",
    "\n",
    "Define: It consists of learning to map input data to known targets (also called annotations)\n",
    "More complicated supervised learning (never heard of the following cases)\n",
    "1. Sequence generation \n",
    "    - Given a picture, predict a caption describing it. Sequence generation can sometimes be reformulated as a series of classification problems (such as repeatedly predicting a word or token in a sequence).\n",
    "2. Syntax tree prediction\n",
    "    - Given a sentence, predict its decomposition into a syntax tree.\n",
    "3. Object detection\n",
    "    - Given a picture, draw a bounding box around certain objects inside the picture. This can also be expressed as a classification problem (given many candidate bounding boxes, classify the contents of each one) or as a joint classification and regression problem, where the bounding-box coordinates are predicted via vector regression.\n",
    "4. Image segmentation\n",
    "    - Given a picture, draw a pixel-level mask on a specific object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Unsupervised learning\n",
    "Since we often deal with unstructured data, we might have to find similarities or some analysis on data. Thus, we often have to figure the underlying data before we even continue (even if we are dealing with supervised data)\n",
    "\n",
    "Dimensionality reduction and clustering are well-known categories of unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Self-supervised learning\n",
    "Within supervised learning, we have self-supervised learning. This type of learning has labels that it is provided (like classification or regression) where we know the answer, but the labels are not created by humans.\n",
    "\n",
    "Instead the labels could come in different forms:\n",
    "- Autoencoders: The label are the input them selves. Remember when we have a movie and we try to find hidden layers to predict the same movie. Hence, the input and output are the movie themselves. Humans did not have to do any of the labeling.\n",
    "\n",
    "Note that the distinction between supervised, self-supervised, and unsupervised learning can be blurry sometimes—these categories are more of a continuum without solid borders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Reinforcement learning\n",
    "In reinforcement learning, an agent receives information about its environment and learns to choose actions that will maximize some reward.\n",
    "\n",
    "Think of DeepMind in Google. They were able to train for the game Go. They would have a lot of games played, that was their input, they would make moves and see the result that their moves would do. For example, if in one game, they realized that a move could be fatal, they would learn this and apply to the next game they play. \n",
    "\n",
    "*Currently, reinforcement learning is mostly a research area and hasn’t yet had significant practical successes beyond games. In time, however, we expect to see reinforcement learning take over an increasingly large range of real-world applications: self-driving cars, robotics, resource management, education, and so on*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and regression glossary:\n",
    "- Sample or input — One data point that goes into your model.\n",
    "- Prediction or output — What comes out of your model.\n",
    "- Target — The truth. What your model should ideally have predicted, according to an external source of data.\n",
    "- Prediction error or loss value — A measure of the distance between your model’s prediction and the target.\n",
    "- Classes — A set of possible labels to choose from in a classification problem. For example, when classifying cat and dog pictures, “dog” and “cat” are the two classes.\n",
    "- Label — A specific instance of a class annotation in a classification problem. For instance, if picture #1234 is annotated as containing the class “dog,” then “dog” is a label of picture #1234.\n",
    "- Binary classification — A classification task where each input sample should be categorized into two exclusive categories.\n",
    "- Multiclass classification — A classification task where each input sample should be categorized into more than two categories: for instance, classifying handwritten digits.\n",
    "- Multilabel classification — A classification task where each input sample can be assigned multiple labels. For instance, a given image may contain both a cat and a dog and should be annotated both with the “cat” label and the “dog” label. The number of labels per image is usually variable\n",
    "- Scalar regression — A task where the target is a continuous scalar value. Predicting house prices is a good example: the different target prices form a continuous space.\n",
    "- Vector regression — A task where the target is a set of continuous values: for example, a continuous vector. If you’re doing regression against multiple values (such as the coordinates of a bounding box in an image), then you’re doing vector regression.\n",
    "- Mini-batch or batch—A small set of samples (typically between 8 and 128) that are processed simultaneously by the model. The number of samples is often a power of 2, to facilitate memory allocation on GPU. When training, a mini-batch is used to compute a single gradient-descent update applied to the weights of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluating machine-learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Training, validation, testing set\n",
    "We have a training set and a validation set which we will train for testing set. Thus, we find the parameters on the training set and tune the model for the validation set (the hyperparameters)\n",
    "-  As a result, tuning the configuration of the model based on its performance on the validation set can quickly result in overfitting to the validation set, even though your model is never directly trained on it.\n",
    "\n",
    "Central to this phenomenon is the notion of information leaks. Every time you tune a hyperparameter of your model based on the model’s performance on the validation set, some information about the validation data leaks into the model.\n",
    "\n",
    "Thus, as we have seen, we train the model on the training set. We then use cross validation to train the validation set. Finally, we predict on our testing set where we do not touch it very much. \n",
    "- At the end of the day, you’ll end up with a model that performs artificially well on the validation data, because that’s what you optimized it for. You care about performance on completely new data, not the validation data, so you need to use a completely different, never-before-seen dataset to evaluate the model: the test dataset. Your model shouldn’t have had access to any information about the test set, even indirectly.\n",
    "\n",
    "**SIMPLE HOLD-OUT VALIDATION**: Set apart some fraction of your data as your test set. Train on the remaining data, and evaluate on the test set.\n",
    "\n",
    "- This is the simplest evaluation protocol, and it suffers from one flaw: if little data is available, then your validation and test sets may contain too few samples to be statistically representative of the data at hand.\n",
    "\n",
    "**K-FOLD VALIDATION**: You will split the data into K parts. Then, depending on how many parts you split (say 10), you will train the model on 9/10 of the dataset. Then, we will have the validation set on the 1/10. Your final score is then the averages of the K scores obtained\n",
    "\n",
    "**ITERATED K-FOLD VALIDATION WITH SHUFFLING** This one is for situations in which you have relatively little data available and you need to evaluate your model as precisely as possible. I’ve found it to be extremely helpful in\n",
    "Kaggle competitions.\n",
    "- It consists of applying K-fold validation multiple times, shuffling the data every time before splitting it K ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Things to keep in mind\n",
    "- Data representativeness — You want both your training set and test set to be representative of the data at hand. For instance, if you’re trying to classify images of digits, and you’re starting from an array of samples where the samples are ordered by their class, taking the first 80% of the array as your training set and the remaining 20% as your test set will result in your training set containing only classes 0–7, whereas your test set contains only classes 8–9. This seems like a ridiculous mistake, but it’s surprisingly common. For this reason, you usually should randomly shuffle your data before splitting it into training and test sets.\n",
    "- The arrow of time — If you’re trying to predict the future given the past (for example, tomorrow’s weather, stock movements, and so on), you should not randomly shuffle your data before splitting it, because doing so will create a temporal leak: your model will effectively be trained on data from the future. In such situations, you should always make sure all data in your test set is posterior to the data in the training set.\n",
    "- Redundancy in your data—If some data points in your data appear twice (fairly common with real-world data), then shuffling the data and splitting it into a training set and a validation set will result in redundancy between the training and validation sets. In effect, you’ll be testing on part of your training data, which is the worst thing you can do! Make sure your training set and validation set are disjoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Data preprocessing, feature engineering, and feature learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Data preprocessing for neural networks\n",
    "**VECTORIZATION**\n",
    "- A key concept is to turn the values into tensors. When the values are in tensors, we are able to computer more efficiently!\n",
    "\n",
    "**VALUE NORMALIZATION**\n",
    "- Also, you must be aware to have the values within the same range. For example, if we look at values  that coul dbe between different features, we could be looking at data that is not within the range. Thus, it could analyze that some values might carry a greater weight while it could be that their value is higher.\n",
    "- Good proxies to follow:\n",
    "    - Take small values—Typically, most values should be in the 0–1 range.\n",
    "    - Be homogenous—That is, all features should take values in roughly the same range.\n",
    "- Advanced proxies to follow:\n",
    "    - Normalize each feature independently to have a mean of 0.\n",
    "    - Normalize each feature independently to have a standard deviation of 1.\n",
    "    - ```python\n",
    "    x -= x.mean(axis=0)\n",
    "    x /= x.std(axis=0)```  \n",
    "    \n",
    "**HANDLING MISSING VALUES**\n",
    "- In general, with neural networks, it’s safe to input missing values as 0, with the condition that 0 isn’t already a meaningful value. The network will learn from exposure to the data that the value 0 means missing data and will start ignoring the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Feature engineering\n",
    "Feature engineering is the process of using your own knowledge about the data and about the machine-learning algorithm at hand (in this case, a neural network) to make the algorithm work better by applying hardcoded (nonlearned) transformations to the data before it goes into the model.\n",
    "\n",
    "Hence, we view that we can apply the learning to have hard-coded features. These features has to be derived from the information that I know. Thus, these generally require my info since the neural networks will nt be able to find the or discover feature enginners.\n",
    "\n",
    "E.g: For example, if we are tyring to find the time that a clock has, we could do the following:\n",
    "- If we go step by step, meaning finding the pixel of the entire image to discover the time, we could have spend a lot of computational forces. Since, the association of the time with the time it could be broken into may pieces. \n",
    "- Now, if we have prior info, where we know that the dark pixels, are the hands, then we could associate the hands with the time and it could learn much quicker\n",
    "- You can go even further: do a coordinate change, and express the (x, y) coordinates as polar coordinates with regard to the center of the image. Your input will become the angle theta of each clock hand.\n",
    "\n",
    "\n",
    "I lied before. Deep leanring has come a long where we find the the info could be derived form deep learning. Fortunately, modern deep learning removes the need for most feature engineering, because neural networks are capable of automatically extracting useful features from raw data. But you should still worry about feature enginnering:\n",
    "1. Good features still allow you to solve problems more elegantly while using fewer resources.\n",
    "2. Good features let you solve a problem with far less data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Overfitting and underfitting\n",
    "Optimization refers to the process of adjusting a model to get the best performance possible on the training data (the learning in machine learning), whereas generalization refers to how well the trained model performs on data it has never seen before.\n",
    "\n",
    "The bad part is that you cannot control the generalization of the model. You only have control to optimize on the training set! Generalization and optimization intially work great but after a while, they tend to go their seperate ways since too much optimization result in performing excellt for the trining set not for the validation set.\n",
    "- Solution: Get more training data. A model trained on more data will naturally generalize better.\n",
    "- Solution: Force the model to only learn on the most prominent patterns. This is called, as you should know, regularization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Reducing the network’s size\n",
    "Reduce the neurons in each layers and/or reduce the total amount of layers!\n",
    "\n",
    "Intuitively, a model with more parameters has more memorization capacity and therefore can easily learn a perfect dictionary-like mapping between training samples and their targets—a mapping without any generalization power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
